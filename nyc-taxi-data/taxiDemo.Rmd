---
title: "Analyzing a billion NYC taxi trips in Spark"
output: html_notebook
---

```{r prep, eval=FALSE, echo=FALSE}
install.packages('leaflet')
install.packages('geosphere')
install.packages('tidyr')
```

# Overview

We analyze the full taxi data as described by [Todd Schneider](http://toddwschneider.com/posts/analyzing-1-1-billion-nyc-taxi-and-uber-trips-with-a-vengeance/) in using R and sparklyr. We load the billion record trips table into Apache Spark and then use sparklyr and dplyr to manipulate the data and run machine learning algorithms at scale.

# Connect

The data represent 200 GB of uncompressed data in CSV format. When converted and compressed in the parquet format, the data are 70 GB. The data are stored in HDFS and pre-loaded in a Hive table.

The Hadoop cluster runs on Elastic Map Reduce (EMR) in AWS and has 12 worker nodes and one master node. The master node has R, RStudio Server Pro, and sparklyr loaded onto it.

Use sparklyr to create a new connection to Apache Spark and cache the trips table. 

```{r connect, echo=FALSE}
library(ggplot2)
library(leaflet)
library(geosphere)
library(tidyr)
library(sparklyr)
library(dplyr)

Sys.setenv(SPARK_HOME="/usr/lib/spark")
config <- spark_config()
sc <- spark_connect(master = "yarn-client", config = config, version = '1.6.1')

tbl_cache(sc, 'trips_csv_2015_12') # 9.6 minute
trips_tbl <- tbl(sc, 'trips_csv_2015_12')
```

# Number of trips

Use dplyr syntax to write Spark SQL. When you're ready to visualize your data, use `collect` to bring data into R memory. Notice the data describe roughly 170 million trips per year.

```{r}
trips_tbl %>% count

trip_by_year <- trips_tbl %>%
  mutate(year = year(pickup_datetime)) %>%
  group_by(year) %>%
  summarize(n = n()) %>%
  collect()

ggplot(trip_by_year, aes(year, n)) + 
  geom_bar(stat="Identity") +
  scale_y_continuous(labels = scales::comma) +
  labs(title = "Number of trips by year", x = "Year", y = "")
```

# Top dropoffs

Spark makes it easy to drill down to any level of you data. You can visualize the top dropoffs for any pickup location by entering your pickup location. In this example the pickup location is JFK International Airport.

```{r}
jfk_pickup_tbl <- trips_tbl %>%
  filter(pickup_nyct2010_gid == 2056) %>%
  filter(!is.na(dropoff_nyct2010_gid)) %>%
  mutate(trip_time = unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) %>%
  group_by(dropoff_nyct2010_gid) %>% 
  summarize(n = n(),
            trip_time_mean = mean(trip_time),
            trip_dist_mean = mean(trip_distance),
            dropoff_latitude = mean(dropoff_latitude),
            dropoff_longitude = mean(dropoff_longitude),
            passenger_mean = mean(passenger_count),
            fare_amount = mean(fare_amount),
            tip_amount = mean(tip_amount))

jfk_pickup <- jfk_pickup_tbl %>%
  mutate(n_rank = min_rank(desc(n))) %>%
  filter(n_rank <= 25) %>%
  collect

leaflet(jfk_pickup) %>% 
  setView(lng = -73.7781, lat = 40.6413, zoom = 11) %>%
  addProviderTiles("CartoDB.Positron") %>%
  addCircleMarkers(~dropoff_longitude, ~dropoff_latitude, stroke = F) %>%
  addCircleMarkers(-73.7781, 40.6413)
```


### Trip duration

You can measure the time between pickup and dropoff for any two locations. In this example, we measure the time between JFK and mid-town. Notice the longest trip times occur around 4 PM.

```{r}
pickup_dropoff_tbl <- trips_tbl %>%
  filter(pickup_nyct2010_gid == 1250 & dropoff_nyct2010_gid == 2056) %>%
  mutate(pickup_hour = hour(pickup_datetime)) %>%
  mutate(trip_time = unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) %>%
  group_by(pickup_hour) %>% 
  summarize(n = n(),
            trip_time_mean = mean(trip_time),
            trip_time_p10 = percentile(trip_time, 0.10),
            trip_time_p25 = percentile(trip_time, 0.25),
            trip_time_p50 = percentile(trip_time, 0.50),
            trip_time_p75 = percentile(trip_time, 0.75),
            trip_time_p90 = percentile(trip_time, 0.90))

pickup_dropoff <- collect(pickup_dropoff_tbl)

ggplot(pickup_dropoff, aes(x = pickup_hour)) +
          geom_line(aes(y = trip_time_p50, alpha = "Median")) +
          geom_ribbon(aes(ymin = trip_time_p25, ymax = trip_time_p75, alpha = "25–75th percentile")) +
          geom_ribbon(aes(ymin = trip_time_p10, ymax = trip_time_p90, alpha = "10–90th percentile")) +
          scale_y_continuous("trip duration in minutes")

```

# Model

With Spark ML you can run machine learning algorithms against all your data in Spark. Here we attempt to understand what factors influence tip amounts.

```{r}
model_tbl <- trips_tbl %>%
  mutate(pickup_hour = hour(pickup_datetime)) %>%
  mutate(pickup_week = weekofyear(pickup_datetime)) %>%
  mutate(pickup_year = year(pickup_datetime)) %>%
  mutate(trip_time = unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) %>%
  filter(!is.na(pickup_nyct2010_gid) & !is.na(dropoff_nyct2010_gid)) %>%
  filter(!is.na(tip_amount)) %>%
  filter(!is.na(fare_amount)) %>%
  filter(!is.na(vendor_id)) %>%
  filter(!is.na(pickup_hour)) %>%
  filter(!is.na(pickup_week)) %>%
  filter(!is.na(passenger_count)) %>%
  filter(!is.na(trip_time)) %>%
  filter(!is.na(trip_distance))

model_partition_tbl <- model_tbl %>%
  filter(pickup_year == 2015) %>%
  sdf_partition(train = 0.5, test = 0.5)

train_tbl <- model_partition_tbl$train
test_tbl <- model_partition_tbl$test

model_formula <- formula(tip_amount ~ 
                           fare_amount + vendor_id + pickup_hour + pickup_week + 
                           passenger_count + trip_time + trip_distance)

timestamp()
m1 <- ml_linear_regression(train_tbl, model_formula) # 13 minutes
timestamp()
summary(m1) # 3 minutes

```
```
Call: ml_linear_regression(train_tbl, model_formula)

Deviance Residuals: (approximate):
       Min         1Q     Median         3Q        Max 
-50.435569  -1.148616   0.009509   0.916313  78.664651 

Coefficients:
                   Estimate  Std. Error   t value Pr(>|t|)    
(Intercept)      4.2660e+00  3.5964e-02  118.6181   <2e-16 ***
fare_amount      1.0479e-01  7.9855e-05 1312.1907   <2e-16 ***
vendor_id_2     -3.1508e-02  1.8909e-03  -16.6634   <2e-16 ***
pickup_hour      3.1155e-03  1.4048e-04   22.1774   <2e-16 ***
pickup_week     -7.7982e-02  7.0792e-04 -110.1562   <2e-16 ***
passenger_count -9.6746e-03  7.2343e-04  -13.3732   <2e-16 ***
trip_time       -1.1711e-07  2.3435e-07   -0.4997   0.6173    
trip_distance   -9.3657e-08  2.7687e-07   -0.3383   0.7352    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-Squared: 0.2165
Root Mean Squared Error: 2.304
```

# Recommendations

It is often useful to visualize the analyses interactively. Use the Shiny app to explore other drill downs and models with the data.

#

First of all, keep him out of the light, he hates bright light, especially sunlight, it'll kill him. Second, don't give him any water, not even to drink. But the most important rule, the rule you can never forget, no matter how much he cries, no matter how much he begs, never feed him after midnight.

First of all, plan time to cache data, it takes a long time to cache data, especially large amounts of data, it'll test your patience. Second, don't collect too much data, not even a modest amount. But the most important rule, the rule you can never forget, no matter how much you need to, no matter how much you want to, never push the stop button.

![gizmo](gizmo.jpg)


