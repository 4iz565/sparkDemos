---
title: "Data Science Toolchain with Spark and R"
subtitle: "Analyzing a billion NYC taxi trips in Spark"
output: 
  html_notebook:
    toc: true
    toc_float: true
---

```{r setup, eval=FALSE, include=FALSE}
install.packages("rsparkling")
install.packages("h2o", type = "source",
                 repos = "http://h2o-release.s3.amazonaws.com/h2o/rel-turnbull/2/R")
```

<center><div style="width:450px">
![R for Data Science http://r4ds.had.co.nz/](http://r4ds.had.co.nz/diagrams/data-science.png)
</div></center>

# Access

It’s rare that a data analysis involves only a single table of data. Typically you have many tables of data, and you must combine them to answer the questions that you’re interested in.

```{r connect, message=FALSE, warning=FALSE, eval=FALSE}
# Load libraries
library(sparklyr)
library(tidyverse)
library(leaflet)
library(rsparkling)
library(h2o)
library(DT)

# Set environ vars
Sys.setenv(SPARK_HOME="/usr/lib/spark")

options(rsparkling.sparklingwater.version = '2.0.3')

# Configure cluster (c3.4xlarge 30G 16core 320disk)
conf <- spark_config()
conf$'sparklyr.shell.executor-memory' <- "20g"
conf$'sparklyr.shell.driver-memory' <- "20g"
conf$spark.executor.cores <- 16
conf$spark.executor.memory <- "20G"
conf$spark.yarn.am.cores  <- 16
conf$spark.yarn.am.memory <- "20G"
conf$spark.executor.instances <- 8
conf$spark.dynamicAllocation.enabled <- "false"
conf$maximizeResourceAllocation <- "true"
conf$spark.default.parallelism <- 32

# Connect to cluster
sc <- spark_connect(master = "yarn-client", config = conf, version = '2.0.0')

# Check H2O
h2o_context(sc)
```

# Understand

## Transform

It is rare that you get the data in exactly the right form you need. Often you’ll need to create some new variables or summaries, or maybe you just want to rename the variables or reorder the observations in order to make the data a little easier to work with.

```{r data, eval = FALSE}
# Load lookup table (via R)
nyct2010_tbl <- read_csv("~/sparkDemos/dev/h2o-demo/nyct2010.csv") %>%
  sdf_copy_to(sc, ., "nyct2010", overwrite = TRUE)

# Load lookup table (via HDFS)
#nyct2010_tbl <- spark_read_csv(sc, "nyct2010", "hdfs:///user/rstudio/nyct2010/")

# Load lookup table (via Hive)
#nyct2010_tbl <- tbl(sc, "nyct2010")

# Join tables
trips_par_tbl <- tbl(sc, "trips_par")
trips_joined_tbl <- trips_par_tbl %>%
  filter(!is.na(pickup_nyct2010_gid) & !is.na(dropoff_nyct2010_gid)) %>%
  filter(cab_type_id %in% c(1, 2)) %>%
  mutate(cab_type = ifelse(cab_type_id == 1, "yellow", "green")) %>%
  mutate(pay_type = ifelse(
    lower(payment_type) %in% c('2', 'csh', 'cash', 'cas'), "cash", ifelse(
      lower(payment_type) %in% c('1', 'crd', 'credit', 'cre'), "credit", "unk"))) %>%
  mutate(other_amount = round(total_amount - fare_amount - tip_amount, 2)) %>%
  left_join(
    select(nyct2010_tbl, pickup_gid = gid, 
           pickup_boro = boroname, pickup_nta = ntaname), 
    by = c("pickup_nyct2010_gid" = "pickup_gid")) %>%
  left_join(
    select(nyct2010_tbl, dropoff_gid = gid, 
           dropoff_boro = boroname, dropoff_nta = ntaname), 
    by = c("dropoff_nyct2010_gid" = "dropoff_gid")) %>%
    select(pickup_datetime, pickup_latitude, pickup_longitude, 
         pickup_nyct2010_gid, pickup_boro, pickup_nta,
         dropoff_datetime, dropoff_latitude, dropoff_longitude, 
         dropoff_nyct2010_gid, dropoff_boro, dropoff_nta,
         cab_type, passenger_count, trip_distance, 
         pay_type, fare_amount, tip_amount, other_amount, total_amount) %>%
  sdf_register("trips_par_joined")

# Save
#spark_write_parquet(trips_joined_tbl, "hdfs:///user/rstudio/trips_model_data")
```

```{r count}
# Calculate total trips
trips_model_data_tbl <- tbl(sc, "trips_model_data")
trips_model_data_tbl %>% count
```

## Visualize

R has several systems for making graphs, but ggplot2 is one of the most elegant and most versatile. ggplot2 implements the grammar of graphics, a coherent system for describing and building graphs. With ggplot2, you can do more faster by learning one system and applying it in many places.

```{r pickups}
source("sqlvis_raster.R")
trips_model_data_tbl %>%
  filter(fare_amount > 0 & fare_amount < 100) %>%
  filter(tip_amount > 0 & tip_amount < 25) %>%
  sqlvis_compute_raster("pickup_longitude", "pickup_latitude") %>%
  sqlvis_ggplot_raster(title = "All Pickups")
```


```{r cutoff}
source("sqlvis_histogram.R")

trips_model_data_tbl %>%
  filter(fare_amount > 0 & fare_amount < 100) %>%
  filter(tip_amount > 0 & tip_amount < 25) %>%
  sqlvis_compute_histogram("tip_amount") %>%
  sqlvis_ggplot_histogram(title = "Tip amount")

trips_model_data_tbl %>%
  filter(fare_amount > 0 & fare_amount < 100) %>%
  filter(tip_amount > 0 & tip_amount < 25) %>%
  filter(pay_type == "credit") %>%
  sqlvis_compute_histogram("fare_amount") %>%
  sqlvis_ggplot_histogram(title = "Fare amount")
```

```{r}
trips_model_data_tbl %>%
  filter(fare_amount > 0 & fare_amount < 100) %>%
  filter(tip_amount > 0 & tip_amount < 25) %>%
  filter(pickup_boro == "Manhattan" & dropoff_boro == "Brooklyn") %>%
  sqlvis_compute_raster("fare_amount", "tip_amount") %>%
  sqlvis_ggplot_raster(title = "Tip and Fare Correlation") -> p

p
p + geom_abline(intercept = 0, 
                slope = c(10,15,20,22,25,27,30,33)/25, 
                col = 'red', alpha = 0.2, size = 1)
```

```{r facets}
trips_model_data_tbl %>%
  filter(fare_amount > 0 & fare_amount < 100) %>%
  filter(tip_amount > 0 & tip_amount < 25) %>%
  sqlvis_compute_raster_g("fare_amount", "tip_amount", "pay_type") %>%
  sqlvis_ggplot_raster_g(title = "Tip and Fare Correlation by Pay Type", ncol = 3)
```


```{r htmlwidgets}
# Summarize trips from JFK Airport
jfk_pickup_tbl <- trips_model_data_tbl %>%
  filter(pickup_nta == "Airport") %>%
  filter(!is.na(dropoff_nyct2010_gid)) %>%
  mutate(trip_time = unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) %>%
  group_by(dropoff_nyct2010_gid) %>% 
  summarize(n = n(),
            trip_time_mean = mean(trip_time),
            trip_dist_mean = mean(trip_distance),
            dropoff_latitude = mean(dropoff_latitude),
            dropoff_longitude = mean(dropoff_longitude),
            passenger_mean = mean(passenger_count),
            fare_amount = mean(fare_amount),
            tip_amount = mean(tip_amount))

# Collect top results
jfk_pickup <- jfk_pickup_tbl %>%
  mutate(n_rank = min_rank(desc(n))) %>%
  filter(n_rank <= 25) %>%
  collect

# Plot top trips on map
leaflet(jfk_pickup) %>% 
  setView(lng = -73.9, lat = 40.7, zoom = 11) %>%
  addProviderTiles("CartoDB.Positron") %>%
  addCircleMarkers(~dropoff_longitude, ~dropoff_latitude, stroke = F, color = "red") %>%
  addCircleMarkers(-73.7781, 40.6413, fill = FALSE, color = "green")
```

## Model

The goal of a model is to provide a simple low-dimensional summary of a dataset. Ideally, the model will capture true *signals* (i.e. patterns generated by the phenomenon of interest), and ignore *noise* (i.e. random variation that you’re not interested in).

## Model Data

```{r counts}
# Select a model data set
model_tbl <- trips_model_data_tbl %>%
  filter(fare_amount > 0 & fare_amount < 100) %>%
  filter(tip_amount >= 0 & tip_amount < 25) %>%
  filter(passenger_count > 0 & passenger_count < 5) %>%
  filter(pickup_nta == "Turtle Bay-East Midtown" & dropoff_nta == "Airport") %>%
  select(tip_amount, fare_amount, pay_type, passenger_count) 

# Partitioin into train and validate
model_partition_tbl <- model_tbl %>%
  sdf_partition(train = 0.1, test = 0.1, seed = 4321)

# Create table references
trips_train_tbl <- sdf_register(model_partition_tbl$train, "trips_train")
trips_test_tbl <- sdf_register(model_partition_tbl$test, "trips_test")

# Cache
tbl_cache(sc, "trips_train")
tbl_cache(sc, "trips_test")
```

## Train ML model

```{r ml_fit}
model_formula <- formula(tip_amount ~ fare_amount + pay_type + passenger_count)
m1 <- ml_linear_regression(trips_train_tbl, model_formula)
summary(m1)
```

### Test ML model

```{r ml_pred}
# Score the predictions
pred_ml_tbl <- sdf_predict(m1, trips_test_tbl)

pred_ml_tbl %>%
  mutate(res = tip_amount - prediction) %>%
  sqlvis_compute_histogram("res") %>%
  sqlvis_ggplot_histogram(title = "Residuals")

pred_ml_tbl %>%
  mutate(residual = tip_amount - prediction) %>%
  sqlvis_compute_raster("prediction", "residual") %>%
  sqlvis_ggplot_raster(title = "Residuals vs Fitted")
```

## H2O Model

```{r h2o_glm}
# Convert to H2O Frames
trips_train_h2o_tbl <- as_h2o_frame(sc, trips_train_tbl)
trips_test_h2o_tbl <- as_h2o_frame(sc, trips_test_tbl)

# Format H2O Frames
trips_train_h2o_tbl$pay_type <- as.factor(trips_train_h2o_tbl$pay_type)
trips_test_h2o_tbl$pay_type <- as.factor(trips_test_h2o_tbl$pay_type)

# Fit model on H2O train data
m2 <- h2o.glm(
  x = c("fare_amount", "pay_type", "passenger_count"), 
  y = "tip_amount", 
  trips_train_h2o_tbl, 
  alpha = 0, 
  lambda = 0
  )
summary(m2)
```

## H2O deep learning

```{r h2o_dl}
m3 <- h2o.deeplearning(
  x = c("fare_amount", "pay_type", "passenger_count"), 
  y = "tip_amount", 
  training_frame = trips_train_h2o_tbl, 
  epochs = 2, 
  seed = 999
  )
summary(m3)
```

## Collect into R

```{r nrow}
# Check number of rows
nrow(trips_train_tbl)
```

```{r collect}
# Collect
trips_train <- collect(trips_train_tbl)
trips_test <- collect(trips_test_tbl)

# Format factors
trips_train$pay_type <- as.factor(trips_train$pay_type)
trips_test$pay_type <- as.factor(trips_test$pay_type)

# Fit linear model
m4 <- glm(model_formula, data = trips_train)
summary(m4)

# Regression tree
library(rpart)
m5 <- rpart(model_formula, trips_train)
summary(m5)
```

## Compare models

```{r compare}
# Score H2O test data
prediction_m2 <- h2o.predict(m2, newdata = trips_test_h2o_tbl)
prediction_m3 <- h2o.predict(m3, newdata = trips_test_h2o_tbl)
pred_h2o <- data.frame(
  tip_amount = trips_test$tip_amount,
  as_spark_dataframe(sc, prediction_m2),
  as_spark_dataframe(sc, prediction_m3)
)

# Score other models
trips_test$pred_lm <- predict(m4, trips_test)
trips_test$pred_rpart <- predict(m5, trips_test)

# Compute the mean squared error
mse <- function(formula, data) {
  data %>%
    mutate_(residual = formula) %>%
    summarize(mse = mean(residual ^ 2)) %>%
    collect %>%
    .[["mse"]]
}

# Summary
out <- data.frame(
  Train = c(
    ML_glm = m1$mean.squared.error,
    H2O_glm = h2o.mse(m2),
    H2O_deeplearning = h2o.mse(m3),
    R_glm = mean(m4$residuals^2),
    R_rpart = mean(resid(m5)^2)
    ),
  Test = c(
    ML_glm = mse(~ tip_amount - prediction, pred_ml_tbl),
    H2O_glm = mse(~ tip_amount - predict, pred_h2o),
    H2O_deeplearning = mse(~ tip_amount - predict.1, pred_h2o),
    R_glm = mse(~ tip_amount - pred_lm, trips_test),
    R_rpart = mse(~ tip_amount - pred_rpart, trips_test)
    )
  )
out
```


# Communicate

After controlling for fare amount, we found that trips paying with cash tip less on average than trips paying with credit. Additionally, trips with more passengers tend to tip less. The linear model fit for H2O is equivalent to the model fit for ML. The deep learning model had the smallest mean square error for both the train and test data.

```{r summary}
out %>%
  mutate(Model = rownames(out)) %>%
  ggplot(aes(x = reorder(Model, Test), y = Test)) + 
  geom_bar(stat = "Identity") + 
  coord_flip() +
  ggtitle("MSE for rest data by model")
```


