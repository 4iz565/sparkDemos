---
title: "Comparison of ML Classifiers Using Sparklyr"
output: 
  html_notebook: default
  html_document:
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
---

# Setup

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
```

```{r}
library(sparklyr)
library(dplyr)
library(tidyr)
library(titanic)
library(ggplot2)
library(purrr)

# Convert titanic_train data into parquet format and output to disk
parquet_path <- "./titanic-parquet"
parquet_table <- "titanic"
if(!dir.exists(parquet_path)){
  sc <- spark_connect(master = "local", version = "2.0.0")
  copy_to(sc, titanic_train, parquet_table, overwrite = TRUE)
  tbl(sc, parquet_table) %>% spark_write_parquet(path = parquet_path)
}
```


# Overview 

This [R Notebook](https://rmarkdown.rstudio.com/r_notebooks.html) will demonstrate how to fit and compare classification models in Spark using the R package [Sparklyr](http://spark.rstudio.com/).

Models will include the Spark ML routines for logistic regression, decision trees, gradient boosted trees, random forest, and multi-layer perceptron (nueral net) and naive bayes.

[results!](#results)

***

# Loading the Data

The analysis will use the popular Titanic kaggle dataset. A thorough background on the dataset and examples of analysis are available [here](https://www.kaggle.com/c/titanic).  The goal is to predict whether an individual survived or died based on factors including their class, gender, age, and family. The titanic data is readily available in R in the titanic package. The dataset is small (891 rows), but the methods presented are applicable to real Spark datasets.

We'll work in a local Spark cluster and read the data in from parquet. The parquet files were generated through the following one-time process:

Given parquet data, it suffices to launch Spark locally and read the parquet files in directly. The analysis is done in Spark 2.0, but most of the analysis can be replicated in earlier versions.

```{r}
sc <- spark_connect(master = "local", version = "2.0.0")
spark_read_parquet(sc, name = "titanic", path = "titanic-parquet")
titanic_tbl <- tbl(sc, "titanic")
```

***

# Data preparation

## Feature engineering with Spark SQL

While this dataset contains some helpful predictions, we'll demo how to create additional features in Sparklyr using dplyr commands.

Variable | Definition
---------|-----------
Family_Size | Number of Siblings and Parents
Pclass | Format as character
Embarked | Remove small number of missing records
Age | Impute missing age with average age

```{r}
titanic2_tbl <- titanic_tbl %>% 
  mutate(Family_Size = SibSp + Parch + 1L) %>% 
  mutate(Pclass = as.character(Pclass)) %>%
  filter(!is.na(Embarked)) %>%
  mutate(Age = if_else(is.na(Age), mean(Age), Age)) %>%
  sdf_register("titanic2")
```

> Sparklyr Tip: `sdf_register` is used to save our table for later analysis.


## Feature Engineering with ML Feature Transformers

Now that age has been added, another feature will be created using one of the Spark ML feature transformers. Specifically, a bucketized variable to represent the age bins: child, teen, adult, and retired.

```{r}
titanic_final_tbl <- titanic2_tbl %>%
  mutate(Family_Size = as.numeric(Family_size)) %>%
  sdf_mutate(
    Family_Sizes = ft_bucketizer(Family_Size, splits = c(1,2,5,12))
    ) %>%
  mutate(Family_Sizes = as.character(as.integer(Family_Sizes))) %>%
  sdf_register("titanic_final")
```

> Sparklry Tip: In a dplyr pipeline, feature transformers are used in conjunction with `sdf_mutate`

## Partition into train and test data

The last step in preparing the data is to define a training and testing partition.

```{r}
partition <- titanic_final_tbl %>% 
  mutate(Survived = as.numeric(Survived), SibSp = as.numeric(SibSp), Parch = as.numeric(Parch)) %>%
  select(Survived, Pclass, Sex, Age, SibSp, Parch, Fare, Embarked, Family_Sizes) %>%
  sdf_partition(train = 0.75, test = 0.25, seed = 8585)

train_tbl <- partition$train
test_tbl <- partition$test
```
> Sparklyr Tip: Use sdf_partition to create training and testing splits.

***

# Model Building

The following section contains code used to construct the different ml models. One of the great features of Sparklyr is that each algorithm accepts R formulas making it easy to create and compare models.

## Train a Logistic Regression Model

```{r}
ml_formula <- formula(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Family_Sizes)

(ml_log <- ml_logistic_regression(train_tbl, ml_formula))
```

## Train Additional Machine Learning Algorithms

```{r}
## Decision Tree
ml_dt <- ml_decision_tree(train_tbl, ml_formula)

## Random Forest
ml_rf <- ml_random_forest(train_tbl, ml_formula)

## Gradient Boosted Tree
ml_gbt <- ml_gradient_boosted_trees(train_tbl, ml_formula)

## Naive Bayes
ml_nb <- ml_naive_bayes(train_tbl, ml_formula)

## Neural Network
ml_nn <- ml_multilayer_perceptron(train_tbl, ml_formula, layers = c(11,15,2))
```

## Score test data

```{r}
ml_models <- list(
  "Logistic" = ml_log,
  "Decision Tree" = ml_dt,
  "Random Forest" = ml_rf,
  "Gradient Boosted Trees" = ml_gbt,
  "Naive Bayes" = ml_nb,
  "Neural Net" = ml_nn
)

score_test_data <- function(model, data=test_tbl){
  pred <- sdf_predict(model, data)
  select(pred, Survived, prediction)
}

ml_score <- lapply(ml_models, score_test_data)
```

***

# Model Comparison 

This section compares the models:

* [Training] Confusion Matrix
* [Feature Importance]
* [Lift Chart]
* [Area Under ROC]
* [Accuracy]

## Lift Chart

A cummulative gains lift chart compares model performance across different portions of the data for a single cutoff value (0.5). This is different from an ROC curve which contains information about model performance for many cutoff values. ROC is harder compute and is not implemented yet for ML algorithms. In a lift chart models that approach the upper left corner perform the best. 

```{r}
calculate_lift <- function(scored_data) {
  scored_data %>%
    mutate(bin = ntile(desc(prediction), 10)) %>% 
    group_by(bin) %>% 
    summarize(count = sum(Survived)) %>% 
    mutate(prop = count / sum(count)) %>% 
    arrange(bin) %>% 
    mutate(prop = cumsum(prop)) %>% 
    select(-count) %>% 
    collect() %>% 
    as.data.frame()
}

ml_gains <- data.frame(bin = 1:10, prop = seq(0, 1, len = 10), model = "Base")

for(i in names(ml_score)){
  ml_gains <- ml_score[[i]] %>%
    calculate_lift %>%
    mutate(model = i) %>%
    rbind(ml_gains, .)
}

ggplot(ml_gains, aes(x = bin, y = prop, colour = model)) +
  geom_point() + geom_line() +
  ggtitle("Lift Chart for Predicting Survival - Test Data Set") + 
  xlab("") + ylab("")
```

> Sparklyr Tip: dplyr and sparklyr both support windows functions, including `ntiles` and `cumsum`.

The lift charts suggests that any of the tree models (random forest, gradient boosted trees, or the decision tree) will provide the best prediction.

## Area under the curve (AUC)

Though ROC curves are not available, Spark ML does have support for Area Under the ROC curve. This metric captures performance across cut-off values, the higher the AUC the better

```{r}
auc <- sapply(ml_score, ml_binary_classification_eval, "Survived", "prediction")

data.frame(model = names(auc), auc = auc) %>%
  ggplot(aes(reorder(model, auc), auc)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  xlab("") +
  ggtitle("Model Area Under ROC")
```

## Accuracy 

```{r}

calc_accuracy <- function(data, cutpoint = 0.5){
  data %>% 
    mutate(prediction = if_else(prediction > cutpoint, 1.0, 0.0)) %>%
    ml_classification_eval("prediction", "Survived", "accuracy")
}

accuracy <- sapply(ml_score, calc_accuracy)

data.frame(models = names(accuracy), accuracy = accuracy, row.names = NULL) %>%
  ggplot(aes(reorder(models, accuracy), accuracy)) + 
    geom_bar(stat = "identity") + 
    coord_flip() +
    xlab("") +
    ggtitle("Model Accuracy")

```

## Confusion Matrix (0.5)

To begin, it is possible to evaluate how well the models work on the training data. For this comparison a confusion matrix is constructed for each model. To create the confusion matrix it is necessary to take the scored probabilites and map them to predictions. 0.5 is used as a cutoff.

```{r}

calc_confusion_matrix <- function(scored_data){
  scored_data %>%
    mutate(pred = as.numeric(if_else(prediction > 0.5, 1, 0))) %>%
    group_by(Survived, pred) %>%
    count %>%
    data.frame %>%
    mutate(n = as.numeric(n)) %>%
    collect
}

ml_confusion <- lapply(ml_score, calc_confusion_matrix)

ml_confusion_summary <- data.frame()
for(i in names(ml_confusion)){
  ml_confusion_summary <- ml_score[[i]] %>%
    calc_confusion_matrix %>%
    mutate(Model = i) %>%
    rbind(ml_confusion_summary, .)
}

ml_confusion_summary %>%
  mutate(e1 = ifelse(Survived == pred, "True", "False")) %>%
  mutate(e2 = ifelse(pred == 1, "Pos", "Neg")) %>%
  mutate(event = paste0(e1, e2)) %>%
  select(Model, event, n) %>%
  spread(event, n) %>%
  mutate(TPR = 1 / (1 + FalseNeg / TruePos)) %>%
  mutate(FPR = 1 / (1 + TrueNeg / FalsePos))
```

All of the models tend to over predict death. This is likely because the training data is not balanced, there were a lot more people who died on the titanic than survived.

## Feature Importance

It is also interesting to compare the features that were identified by each model as being important predictors for survival. The logistic regression and tree models in Spark 2.0.0 implement feature importance metrics. The relative importance of each feature is compared below by model.

```{r warning = FALSE}

feature_importance <- data.frame()

for(i in c("Decision Tree", "Random Forest", "Gradient Boosted Trees")){
  feature_importance <- ml_tree_feature_importance(sc, ml_models[[i]]) %>%
    mutate(Model = i) %>%
    mutate(importance = as.numeric(levels(importance))[importance]) %>%
    mutate(feature = as.character(feature)) %>%
    rbind(feature_importance, .)
}

feature_importance %>%
  ggplot(aes(x = Model, y = feature, colour = importance, size = importance)) +
  geom_point() + scale_size(guide = FALSE)
```

The number of siblings aboard and gender appear to be the most influential.


Arguably the most important comparison is how the models perform outside of the training data. Three metrics are used to compare model performance on the testing dataset: lift, Area Under the ROC curve, and accuracy.



***

## Training Time

Even with a samll dataset the time to train an ml algorithm is important. The following code trains each model `n` times and plots the results. (Training the multi-layer perceptron model took noticeably longer and was not included)

```{r warning = FALSE}

n <- 10

deparse_call <- function(x){
  x <- paste(deparse(x), collapse = "")
  x <- gsub("\\s+", " ", paste(x, collapse = ""))
  x
}

format_call <- function(y){
  y <- deparse_call(y[[".call"]])
  y <- gsub('ml_formula', ml_formula_char, y)
  y <- paste0("system.time(", y, ")")
  y
}

ml_formula_char <- deparse_call(ml_formula)

model_call <- sapply(ml_models, format_call) %>%
  rep(., n) %>%
  parse(text = .)

res  <- map(model_call, eval)

result <- data.frame(model = rep(names(ml_models), n),
                     time = sapply(res, function(x){as.numeric(x["elapsed"])})) 

result %>% ggplot(aes(time,model)) + 
  geom_boxplot() + 
  geom_jitter(width = 0.4, aes(colour = model)) +
  scale_colour_discrete(guide = FALSE)
```

